diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 1df6da00..05c9aa33 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -2262,7 +2262,7 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 	/*
 	 * Make sure we do not leak PI boosting priority to the child.
 	 */
-	p->prio = current->normal_prio;
+	//p->prio = current->normal_prio;
 
 	/*
 	 * Revert to default priority/policy on fork if requested.
@@ -2307,6 +2307,14 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 	raw_spin_lock_irqsave(&p->pi_lock, flags);
 	set_task_cpu(p, cpu);
 	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
+	
+	if (p->policy == SCHED_NORMAL) {
+		p->prio = current->normal_prio - NICE_WIDTH - PRIO_TO_NICE(current->static_prio);
+		p->normal_prio = p->prio;
+		p->rt_priority = p->prio;
+		p->policy = SCHED_FIFO;
+		p->static_prio = NICE_TO_PRIO(0);
+	}
 
 #ifdef CONFIG_SCHED_INFO
 	if (likely(sched_info_on()))
@@ -4230,6 +4238,11 @@ static int _sched_setscheduler(struct task_struct *p, int policy,
 		attr.sched_policy = policy;
 	}
 
+	if (attr.sched_policy == SCHED_NORMAL) {
+		attr.sched_priority = param->sched_priority - NICE_WIDTH - attr.sched_nice;
+		attr.sched_policy = SCHED_FIFO;
+	}
+
 	return __sched_setscheduler(p, &attr, check, true);
 }
 /**
diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c
index 541b8494..2d6ee020 100644
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -2319,6 +2319,7 @@ static void task_tick_rt(struct rq *rq, struct task_struct *p, int queued)
 	 * RR tasks need a special form of timeslice management.
 	 * FIFO tasks have no timeslices.
 	 */
+	p->rt.time_slice = sched_rr_timeslice;
 	if (p->policy != SCHED_RR)
 		return;
 
